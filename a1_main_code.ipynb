{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP-8730_Assignment-1 Spell Correction Using MED\n",
        ">## Student Information\n",
        ">* Name: Jiajie Yang\n",
        ">* UWin Acc: yang4q\n",
        ">* Student ID: 110115897"
      ],
      "metadata": {
        "id": "h-fJv_7nYGfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!pip install \"pytrec-eval-terrier\""
      ],
      "metadata": {
        "id": "2KeQvXqPYt_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209778ed-df21-42f5-c3a1-63d49ac7afad"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytrec-eval-terrier in /usr/local/lib/python3.8/dist-packages (0.5.5)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OAsJ5VRh9LpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e371cfdd-af5a-435d-a098-bbd70b0ce0e5"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "#assert(nltk.download('wordnet'))\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy\n",
        "import heapq"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Levenshtein Distance Pseudocode\n",
        "> **Algorithm** Min-Edit-Distance;\\\n",
        "> **Input:** Strings of source and target\\\n",
        "> **Output:** Minimum distance between source and target\n",
        "\n",
        "```\n",
        "n <- Len(source)\n",
        "m <- Len(target)\n",
        "Create a distance matrix, distance[n+1, m+1]\n",
        "\n",
        "Initialization: the first row and column is the distance between the empty string and itself\n",
        "D[0,0] = 0\n",
        "for i from 1 -> n do:\n",
        "    D[i,0] <- D[i-1,0] + del-cost(source[i])\n",
        "for j from 1 -> m do:\n",
        "    D[0,j] <- D[0,j-1] + ins-cost(target[j])\n",
        "\n",
        "Recurrence relation:\n",
        "for i from 1 -> n do:\n",
        "    for j from 1 -> m do:\n",
        "        D[i,j] <- Min(D[i-1,j] + del-cost(source[i]),\n",
        "                      D[i-1,j-1] + sub-cost(source[i],target[j]),\n",
        "                      D[i,j-1] + ins-cost(target[j])\n",
        "\n",
        "return D[n,m]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BF3j_tdNfo0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_distance(src, tar):\n",
        "    n = len(src)\n",
        "    m = len(tar)\n",
        "    distances = numpy.zeros((n + 1, m + 1))\n",
        "\n",
        "    for i in range(n + 1):\n",
        "        distances[i][0] = i\n",
        "\n",
        "    for j in range(m + 1):\n",
        "        distances[0][j] = j\n",
        "    \n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            if (src[i-1] == tar[j-1]):\n",
        "                distances[i][j] = distances[i - 1][j - 1]\n",
        "            else:\n",
        "                distances[i][j] = 1 + min(distances[i - 1][j],     # delete\n",
        "                                          distances[i - 1][j - 1], # substitute\n",
        "                                          distances[i][j - 1])     # insert\n",
        "    \n",
        "    return distances[n][m]\n",
        "\n",
        "# Tests:\n",
        "assert min_edit_distance(\"c\", \"cu\") == 1\n",
        "assert min_edit_distance(\"cut\", \"ct\") == 1\n",
        "assert min_edit_distance(\"arccot\", \"arccos\") == 1"
      ],
      "metadata": {
        "id": "k_VLUTJptxDq"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Given Incorrect Spell to Find Top-K Similar Words\n",
        "> **Goal:** We want to find the k smallest minimum edit distance from the given incorrect spell among all words in the dictionary D by using a max heap \\\n",
        "> **Algorithm** Find-Top-K-Similar;\\\n",
        "> **Input:** Incorrect spell string, inc_str; integer, k\\\n",
        "> **Output:** A list of most similar words of length k"
      ],
      "metadata": {
        "id": "OYfKL8PlFV1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Customized Max Heap\n",
        "class MaxHeap_MED_Word:\n",
        "    \n",
        "    def __init__(self, maxsize):\n",
        "        \n",
        "        self.maxsize = maxsize\n",
        "        self.size = 0\n",
        "        self.Heap = [[]] * self.maxsize\n",
        "        self.Root = 0\n",
        "    \n",
        "    # Function to return True if self is an empty heap; False, otherwise\n",
        "    def is_empty(self):\n",
        "\n",
        "        return self.size == 0\n",
        "\n",
        "    # Function to return True if self is full; False, otherwise\n",
        "    def is_full(self):\n",
        "\n",
        "        return self.size == self.maxsize\n",
        "\n",
        "    # Function to return the index of the last index\n",
        "    def last(self):\n",
        "\n",
        "        return self.size - 1\n",
        "        \n",
        "    # Function to return the position of parent for the node currently at pos\n",
        "    def parent(self, pos):\n",
        "        \n",
        "        return (pos - 1) // 2\n",
        "\n",
        "    # Function to return the position of the left child for the node currently at pos\n",
        "    def leftChild(self, pos):\n",
        "        \n",
        "        return 2 * pos + 1\n",
        "\n",
        "    # Function to return the position of the right child for the node currently at pos\n",
        "    def rightChild(self, pos):\n",
        "        \n",
        "        return 2 * (pos + 1)\n",
        "    \n",
        "    # Function to return True if pos has left child; False, otherwise\n",
        "    def has_leftChild(self, pos):\n",
        "\n",
        "        return self.leftChild(pos) < self.size;\n",
        "\n",
        "    # Function to return True if pos has right child; False, otherwise\n",
        "    def has_rightChild(self, pos):\n",
        "        \n",
        "        return self.rightChild(pos) < self.size;\n",
        "\n",
        "    # Function that returns true if the passed node is a leaf node\n",
        "    def isLeaf(self, pos):\n",
        "        \n",
        "        return self.leftChild(pos) >= self.size and pos < self.size\n",
        "\n",
        "    # Function that returns true if the passed node is a leaf node\n",
        "    def isRoot(self, pos):\n",
        "        \n",
        "        return pos == 0\n",
        "\n",
        "    # Function to swap two nodes of the heap\n",
        "    def swap(self, pos_1, pos_2):\n",
        "        \n",
        "        tmp = self.Heap[pos_1]\n",
        "        self.Heap[pos_1] = self.Heap[pos_2]\n",
        "        self.Heap[pos_2] = tmp\n",
        "        return\n",
        "\n",
        "    # Function to heapify the node at pos\n",
        "    def maxHeapify(self, pos):\n",
        "\n",
        "        while not self.isLeaf(pos):\n",
        "            large_idx = self.leftChild(pos)\n",
        "            if self.has_rightChild(pos) and (self.Heap[self.rightChild(pos)][0] >\n",
        "                                             self.Heap[large_idx][0]):\n",
        "                large_idx = self.rightChild(pos)\n",
        "            if self.Heap[pos][0] >= self.Heap[large_idx][0]:\n",
        "                break\n",
        "            self.swap(pos, large_idx)\n",
        "            pos = large_idx\n",
        "        return\n",
        "\n",
        "    # Function to insert a node into the heap\n",
        "    def insert(self, element):\n",
        "        \n",
        "        if self.size >= self.maxsize:\n",
        "            return\n",
        "        self.size += 1\n",
        "        self.Heap[self.size - 1] = element\n",
        "\n",
        "        current = self.size -1\n",
        "\n",
        "        while (not self.isRoot(current) and \n",
        "               self.Heap[current][0] > self.Heap[self.parent(current)][0]):\n",
        "            self.swap(current, self.parent(current))\n",
        "            current = self.parent(current)\n",
        "        return\n",
        "    \n",
        "    # Function to return the maximum element from the heap\n",
        "    def top(self):\n",
        "\n",
        "        return self.Heap[self.Root]\n",
        "    \n",
        "    # Function to return the maximum element's key from the heap\n",
        "    def topKey(self):\n",
        "\n",
        "        return self.Heap[self.Root][0]\n",
        "\n",
        "    # Function to remove and return the maximum element from the heap\n",
        "    def extractMax(self):\n",
        "\n",
        "        popped = self.Heap[self.Root]\n",
        "        self.Heap[self.Root] = self.Heap[self.last()]\n",
        "        self.size -= 1\n",
        "        if not self.is_empty():\n",
        "            self.maxHeapify(self.Root)\n",
        "        \n",
        "        return popped\n",
        "\n",
        "    # Function to print the contents of the heap\n",
        "    def Print(self):\n",
        "        \n",
        "        for i in range(0, self.size):\n",
        "            sl = self.Heap[i]\n",
        "            if self.has_rightChild(i):\n",
        "                lc = self.Heap[self.leftChild(i)]\n",
        "                rc = self.Heap[self.rightChild(i)]\n",
        "                print(\"PARENT-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] +\n",
        "                          \") ->LEFT CHILD: (\" + str(lc[0]) + \",\" + lc[1] +\n",
        "                          \") ->RIGHT CHILD: (\" + str(rc[0]) + \", \" + rc[1] + \")\")\n",
        "            elif self.has_leftChild(i):\n",
        "                lc = self.Heap[self.leftChild(i)]\n",
        "                print(\"PARENT-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] +\n",
        "                          \") ->LEFT CHILD: (\" + str(lc[0]) + \",\" + lc[1] + \")\")\n",
        "            elif self.isLeaf(i):\n",
        "                print(\"PARENT(Leaf)-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] + \")\")\n",
        "\n",
        "# Tests:\n",
        "tuple1 = (99, \"worldsmall\")\n",
        "tuple2 = (990, \"worldbig\")\n",
        "tuple3 = (32, \"Msmall\")\n",
        "tuple4 = (46, \"Mbig\")\n",
        "tuple5 = (120, \"BigSS\")\n",
        "maxHeap = MaxHeap_MED_Word(4)\n",
        "maxHeap.insert(tuple1)\n",
        "maxHeap.insert(tuple2)\n",
        "maxHeap.insert(tuple3)\n",
        "maxHeap.insert(tuple4)\n",
        "#maxHeap.Print()\n",
        "print(\"The Max val is (\" + str(maxHeap.top()[0]) + \", \" + maxHeap.top()[1] + \")\")\n",
        "maxHeap.extractMax()\n",
        "maxHeap.insert(tuple5)\n",
        "#maxHeap.Print()\n",
        "print(\"The Max val is (\" + str(maxHeap.top()[0]) + \", \" + maxHeap.top()[1] + \")\")\n",
        "maxHeap.extractMax()\n",
        "#maxHeap.Print()\n",
        "print(\"The Max val is (\" + str(maxHeap.top()[0]) + \", \" + maxHeap.top()[1] + \")\")\n",
        "maxHeap.extractMax()\n",
        "#maxHeap.Print()\n",
        "print(\"The Max val is (\" + str(maxHeap.top()[0]) + \", \" + maxHeap.top()[1] + \")\")\n",
        "maxHeap.Print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53so67CIXuJ1",
        "outputId": "1610419b-13de-4da9-d17f-21d7f82e195e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Max val is (990, worldbig)\n",
            "The Max val is (120, BigSS)\n",
            "The Max val is (99, worldsmall)\n",
            "The Max val is (46, Mbig)\n",
            "PARENT-0: (46,Mbig) ->LEFT CHILD: (32,Msmall)\n",
            "PARENT(Leaf)-1: (32,Msmall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_k_similar(inc_str, k):\n",
        "    result = MaxHeap_MED_Word(k) # result is a k-element max heap\n",
        "    for synset in wn.words():\n",
        "        word = synset\n",
        "        cur_tuple = (min_edit_distance(word, inc_str), word)\n",
        "        if not result.is_full():\n",
        "            result.insert(cur_tuple)\n",
        "        elif cur_tuple[0] < result.topKey():\n",
        "            result.extractMax()\n",
        "            result.insert(cur_tuple)\n",
        "    return result\n",
        "\n",
        "def find_top_k_similar_using_synset(inc_str, k):\n",
        "    result = MaxHeap_MED_Word(k) # result is a k-element max heap\n",
        "    for synset in list(wn.all_synsets(lang='eng')):\n",
        "        word = synset.name().split(\".\")[0]\n",
        "        cur_tuple = (min_edit_distance(word, inc_str), word)\n",
        "        if not result.is_full():\n",
        "            result.insert(cur_tuple)\n",
        "        elif cur_tuple[0] < result.topKey():\n",
        "            result.extractMax()\n",
        "            result.insert(cur_tuple)\n",
        "    return result\n",
        "\n",
        "# Tests:\n",
        "def test_find_top_k():\n",
        "    r1 = find_top_k_similar_using_synset(\"saalt\", 1)\n",
        "    r2 = find_top_k_similar_using_synset(\"sag\", 2)\n",
        "    r3 = find_top_k_similar_using_synset(\"desinged\", 10)\n",
        "    i = 0\n",
        "    r3.Print()\n",
        "    while not r3.is_empty():\n",
        "        popped = r3.extractMax()\n",
        "        print(\"r3[\" + str(i) + \"]: (\" + str(popped[0]) + \",\" + popped[1] + \")\")\n",
        "        i = i + 1\n",
        "\n",
        "    r3_using_words = find_top_k_similar(\"saalt\", 10)\n",
        "    i = 0\n",
        "    while not r3_using_words.is_empty():\n",
        "        popped = r3_using_words.extractMax()\n",
        "        print(\"r3_words[\" + str(i) + \"]: (\" + str(popped[0]) + \",\" + popped[1] + \")\")\n",
        "        i = i + 1"
      ],
      "metadata": {
        "id": "5QTXiN64J5EA"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Compute Average S@K in Birkbeck Corpus\n",
        "> **Method 1:** Each iteration, we find the correct word with a prefix of '$' and calculate the s@k of the following incorrect words. Then, we record s@k's in a list of three elements for each k = {1, 5, 10} and calculate the averages finally"
      ],
      "metadata": {
        "id": "-WIlP1kq4Lhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def avg_sak(lst_k, path):\n",
        "    file_ = open(path, 'r')\n",
        "    lines = file_.readlines()\n",
        "\n",
        "    # Init total_sak_lst to be list of list of zero's to represent\n",
        "    #     list of [# of successes, # of failures]\n",
        "    total_sak_lst = []\n",
        "    for elem in lst_k:\n",
        "        total_sak_lst.append([0,0])\n",
        "\n",
        "    largest_k = lst_k[-1] # the largest k in the list of all k values\n",
        "    top_largest_k_lst = [\"\"] * largest_k\n",
        "\n",
        "    word_cor = \"\"\n",
        "    word_inc_lst = []\n",
        "    for l in lines:\n",
        "        if '$' in l:\n",
        "            if not word_cor == \"\" and not len(word_inc_lst) == 0:\n",
        "                for inc_word in word_inc_lst:\n",
        "                    top_largest_k_heap = find_top_k_similar(inc_word, largest_k)\n",
        "                    for i in range(1, largest_k + 1):\n",
        "                        top_largest_k_lst[largest_k - i] = top_largest_k_heap.extractMax()[1]\n",
        "                    for i in range(0, len(lst_k)):\n",
        "                        if word_cor in top_largest_k_lst[:(lst_k[i])]:\n",
        "                            total_sak_lst[i][0] += 1\n",
        "                        else:\n",
        "                            total_sak_lst[i][1] += 1\n",
        "                word_cor = \"\"\n",
        "                word_inc_lst = []\n",
        "            word_cor = l.split(\"$\")[1].split(\"\\n\")[0].lower()\n",
        "        else:\n",
        "            word_inc_lst.append(l.split(\"\\n\")[0].lower())\n",
        "    return total_sak_lst\n",
        "\n",
        "# Test:\n",
        "def test_avg_sak():\n",
        "    print(avg_sak([1,5,10], '/content/birkbeck.dat'))\n"
      ],
      "metadata": {
        "id": "Na3aQLrX66eS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Method 2:** Use the API, pytrec-eval-terrier"
      ],
      "metadata": {
        "id": "Gp6-GILoJex4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytrec_eval\n",
        "import json\n",
        "\n",
        "def eval_success(path):\n",
        "    file_ = open(path, 'r')\n",
        "    lines = file_.readlines()\n",
        "\n",
        "    avg_success = [0,0,0]\n",
        "    qrel = {}\n",
        "    run = {}\n",
        "\n",
        "    lst_k = [1, 5, 10]\n",
        "    largest_k = lst_k[-1]\n",
        "    word_cor = \"\"\n",
        "    for l in lines:\n",
        "        if '$' in l:\n",
        "            word_cor = l.split(\"$\")[1].split(\"\\n\")[0].lower()\n",
        "        else:\n",
        "            word_inc = l.split(\"\\n\")[0].lower()\n",
        "            qrel[word_inc] = {word_cor : 1}\n",
        "            run[word_inc] = {}\n",
        "            # Find the k-closest wrods\n",
        "            top_largest_k_heap = find_top_k_similar(word_inc, largest_k)\n",
        "            for i in range(1, len(lst_k) + 1):\n",
        "                cut_off = lst_k[len(lst_k) - i]\n",
        "                key_val = 1 / cut_off\n",
        "                interval = cut_off\n",
        "                if not i == len(lst_k):\n",
        "                    interval -= lst_k[len(lst_k) - i - 1]\n",
        "                for j in range(interval):\n",
        "                    predict = top_largest_k_heap.extractMax()[1]\n",
        "                    run[word_inc][predict] = key_val\n",
        "    # Apply evaluation measures\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'success'})\n",
        "    print(json.dumps(evaluator.evaluate(run), indent=2))\n",
        "    res = evaluator.evaluate(run)\n",
        "    print('Averages:')\n",
        "    idx = 0\n",
        "    for measure in list(res[list(res.keys())[0]].keys()):\n",
        "        q = [query_measures[measure] for query_measures in res.values()]\n",
        "        val = pytrec_eval.compute_aggregated_measure(measure, q)\n",
        "        avg_success[idx] = val\n",
        "        idx += 1\n",
        "        print(\"  \", measure, val)\n",
        "    return avg_success\n",
        "\n",
        "eval_success('/content/birkbeck.dat')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV8wzYFiJ5sZ",
        "outputId": "860087b0-064e-4543-bb4d-b87a26fd8f32"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ab\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 0.0\n",
            "  },\n",
            "  \"ameraca\": {\n",
            "    \"success_1\": 1.0,\n",
            "    \"success_5\": 1.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"amercia\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 1.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"ameracan\": {\n",
            "    \"success_1\": 1.0,\n",
            "    \"success_5\": 1.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"apirl\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 0.0\n",
            "  },\n",
            "  \"austrain\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"badcock\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 0.0\n",
            "  },\n",
            "  \"bechuarnia_land\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 0.0\n",
            "  },\n",
            "  \"botuania\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"cambrige\": {\n",
            "    \"success_1\": 1.0,\n",
            "    \"success_5\": 1.0,\n",
            "    \"success_10\": 1.0\n",
            "  },\n",
            "  \"canda\": {\n",
            "    \"success_1\": 0.0,\n",
            "    \"success_5\": 0.0,\n",
            "    \"success_10\": 1.0\n",
            "  }\n",
            "}\n",
            "Averages:\n",
            "   success_1 0.2727272727272727\n",
            "   success_5 0.36363636363636365\n",
            "   success_10 0.6363636363636364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2727272727272727, 0.36363636363636365, 0.6363636363636364]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Parallel Execution\n",
        "> **Idea:** Since the spell correction is context-free, we have that the probabilty of successfully correcting a spelling error is independent from other errors. We can divide the large spelling error corpus $C$ into $C_1, C_2, ..., C_n$, then apply the same algorithm, eval_success() on $C_i\\forall i$ on different cores and run simultaneously. If we evenly divide $C$ such that $|C_1|= |C_2|= ... = |C_n|$, ideally we can reduce the time complexity by a factor of n."
      ],
      "metadata": {
        "id": "f-fVcnQb1U6A"
      }
    }
  ]
}